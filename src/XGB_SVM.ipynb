{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"../lib/\")\n",
    "from preprocessing import features, data_splitting, data_resampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'train_preprocessed_type_of_loan.csv'\n",
    "\n",
    "df = pd.read_csv(\"../Data/\" + file, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s, a, n = (1, 1, 1, 0)\n",
    "tmp, num, cat = features(df, n, a)\n",
    "X_train, X_test, y_train, y_test = data_splitting(tmp, cat, num, m)\n",
    "X_train, y_train = data_resampler(X_train, y_train, s)\n",
    "\n",
    "cat_preprocessor = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                            (\"onehot\", OneHotEncoder(sparse=True, handle_unknown=\"ignore\"))])\n",
    "\n",
    "num_preprocessor = Pipeline([\n",
    "                            (\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"numerical\", num_preprocessor, num),\n",
    "    (\"categorical\", cat_preprocessor, cat)\n",
    "])\n",
    "\n",
    "## Transforming\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "try:\n",
    "    X_train = scaler.fit_transform(X_train).toarray()\n",
    "    X_test = scaler.transform(X_test).toarray()\n",
    "except:\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction to Support Vector Machines**\n",
    "\n",
    "**Support Vector Machines** (SVMs in short) are machine learning algorithms that are used for classification and regression purposes. SVMs are one of the powerful machine learning algorithms for classification, regression and outlier detection purposes. An SVM classifier builds a model that assigns new data points to one of the given categories. Thus, it can be viewed as a non-probabilistic binary linear classifier.\n",
    "\n",
    "The original SVM algorithm was developed by Vladimir N Vapnik and Alexey Ya. Chervonenkis in 1963. At that time, the algorithm was in early stages. The only possibility is to draw hyperplanes for linear classifier. In 1992, Bernhard E. Boser, Isabelle M Guyon and Vladimir N Vapnik suggested a way to create non-linear classifiers by applying a kernel function to maximum-margin hyperplanes. The current standard was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.\n",
    "\n",
    "SVMs can be used for linear classification purposes. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using a **kernel**. It enable us to implicitly map the inputs into high dimensional feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines intuition\n",
    "\n",
    "### Hyperplane\n",
    "\n",
    "A hyperplane is a decision boundary which separates between given set of data points having different class labels. The SVM classifier separates data points using a hyperplane with the maximum amount of margin. This hyperplane is known as the `maximum margin hyperplane` and the linear classifier it defines is known as the `maximum margin classifier`.\n",
    "\n",
    "\n",
    "### Support Vectors\n",
    "\n",
    "Support vectors are the sample data points, which are closest to the hyperplane.  These data points will define the separating line or hyperplane better by calculating margins.\n",
    "\n",
    "\n",
    "### Margin\n",
    "\n",
    "A margin is a separation gap between the two lines on the closest data points. It is calculated as the perpendicular distance from the line to support vectors or closest data points. In SVMs, we try to maximize this separation gap so that we get maximum margin.\n",
    "\n",
    "The following diagram illustrates these concepts visually.\n",
    "\n",
    "\n",
    "### Margin in SVM\n",
    "\n",
    "<p align='center'>\n",
    "<img src='https://static.wixstatic.com/media/8f929f_7ecacdcf69d2450087cb4a898ef90837~mv2.png'>\n",
    "</p>\n",
    "\n",
    "### SVM Under the hood\n",
    "\n",
    "In SVMs, our main objective is to select a hyperplane with the maximum possible margin between support vectors in the given dataset. SVM searches for the maximum margin hyperplane in the following 2 step process –\n",
    "\n",
    "\n",
    "1.\tGenerate hyperplanes which segregates the classes in the best possible way. There are many hyperplanes that might classify the data. We should look for the best hyperplane that represents the largest separation, or margin, between the two classes.\n",
    "\n",
    "2.\tSo, we choose the hyperplane so that distance from it to the support vectors on each side is maximized. If such a hyperplane exists, it is known as the **maximum margin hyperplane** and the linear classifier it defines is known as a **maximum margin classifier**. \n",
    "\n",
    "\n",
    "The following diagram illustrates the concept of **maximum margin** and **maximum margin hyperplane** in a clear manner.\n",
    "\n",
    "\n",
    "### Maximum margin hyperplane\n",
    "<p align='center'><img src='https://static.packt-cdn.com/products/9781783555130/graphics/3547_03_07.jpg'></p>\n",
    "\n",
    "### Problem with dispersed datasets\n",
    "\n",
    "Sometimes, the sample data points are so dispersed that it is not possible to separate them using a linear hyperplane. \n",
    "In such a situation, SVMs uses a `kernel trick` to transform the input space to a higher dimensional space as shown in the diagram below. It uses a mapping function to transform the 2-D input space into the 3-D input space. Now, we can easily segregate the data points using linear separation.\n",
    "\n",
    "\n",
    "### Kernel trick - transformation of input space to higher dimensional space\n",
    "<p align='center'><img src='http://www.aionlinecourse.com/uploads/tutorials/2019/07/11_21_kernel_svm_3.png'></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Kernel Method**\n",
    "\n",
    "In practice, SVM algorithm is implemented using a `kernel`. It uses a technique called the `kernel trick`. In simple words, a `kernel` is just a function that maps the data to a higher dimension where data is separable. A kernel transforms a low-dimensional input data space into a higher dimensional space. So, it converts non-linear separable problems to linear separable problems by adding more dimensions to it. Thus, the kernel trick helps us to build a more accurate classifier. Hence, it is useful in non-linear separation problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a kernel function as follows\n",
    "$$K(\\bar{x})= \\left\\{\\begin{matrix}\n",
    "1 & if \\left\\| \\bar{x}\\right\\|\\leq 1 \\\\\n",
    "0 &  otherwise\\\\\n",
    "\\end{matrix}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of SVMs, there are 4 popular kernels: `Linear kernel`, `Polynomial kernel`, `Radial Basis Function (RBF) kernel` (also called Gaussian kernel) and `Sigmoid kernel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Linear kernel**\n",
    "\n",
    "In linear kernel, the kernel function takes the form of a linear function as follows-\n",
    "\n",
    "**linear kernel : K($x_i$ , $x_j$) = $x_i^T$ $x_j$**\n",
    "\n",
    "Linear kernel is used when the data is linearly separable. It means that data can be separated using a single line. It is one of the most common kernels to be used. It is mostly used when there are large number of features in a dataset. Linear kernel is often used for text classification purposes.\n",
    "\n",
    "Training with a linear kernel is usually faster, because we only need to optimize the C regularization parameter. When training with other kernels, we also need to optimize the γ parameter. So, performing a grid search will usually take more time.\n",
    "\n",
    "Linear kernel can be visualized with the following figure\n",
    "<p align=\"center\">\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_svm_kernels_thumb.png'>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Polynomial Kernel**\n",
    "\n",
    "Polynomial kernel represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables. The polynomial kernel looks not only at the given features of input samples to determine their similarity, but also combinations of the input samples.\n",
    "\n",
    "For degree-d polynomials, the polynomial kernel is defined as follows –\n",
    "\n",
    "**Polynomial kernel : $K(x_i , x_j) = (\\gamma x_i^T x_j + r)^d , \\gamma > 0$**\n",
    "\n",
    "Polynomial kernel is very popular in Natural Language Processing. The most common degree is d = 2 (quadratic), since larger degrees tend to overfit on NLP problems. It can be visualized with the following diagram.\n",
    "\n",
    "<p align='center'><img src='https://www.researchgate.net/profile/Cheng_Soon_Ong/publication/23442384/figure/fig12/AS:341444054274063@1458418014823/The-effect-of-the-degree-of-a-polynomial-kernel-The-polynomial-kernel-of-degree-1-leads.png'></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Radial Basis Function Kernel**\n",
    "\n",
    "Radial basis function kernel is a general purpose kernel. It is used when we have no prior knowledge about the data. The RBF kernel on two samples x and y is defined by the following equation \n",
    "$$K(x, y) = \\exp\\left ( -\\frac{\\left\\|x-y \\right\\|^2}{2\\sigma^2} \\right ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sigmoid kernel**\n",
    "\n",
    "Sigmoid kernel has its origin in neural networks. We can use it as the proxy for neural networks. Sigmoid kernel is given by the following equation $$K(x, y) = \\tanh \\left ( \\alpha x^Ty + c \\right )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.79     42273\n",
      "           1       0.60      0.75      0.67     34394\n",
      "           2       0.86      0.72      0.78     51022\n",
      "\n",
      "    accuracy                           0.75    127689\n",
      "   macro avg       0.75      0.75      0.75    127689\n",
      "weighted avg       0.77      0.75      0.75    127689\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(model.predict(X_train), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.67      0.71      6637\n",
      "           1       0.59      0.86      0.70      7345\n",
      "           2       0.84      0.50      0.63      6018\n",
      "\n",
      "    accuracy                           0.69     20000\n",
      "   macro avg       0.73      0.68      0.68     20000\n",
      "weighted avg       0.73      0.69      0.68     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(model.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is prohibitive to use SVM for this particular dataset, because the SVM training time is $O(n^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree algorithm terminology <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "Before we dive into how a decision tree worksExternal link:open_in_new, let’s define some key terms of a decision tree.\n",
    "- Root node: The base of the decision tree.\n",
    "- Splitting: The process of dividing a node into multiple sub-nodes.\n",
    "- Decision node: When a sub-node is further split into additional sub-nodes.\n",
    "- Leaf node: When a sub-node does not further split into additional sub-nodes; represents possible outcomes.\n",
    "- Pruning: The process of removing sub-nodes of a decision tree.\n",
    "- Branch: A subsection of the decision tree consisting of multiple nodes.\n",
    "\n",
    "Let’s take a look at an example for this. You’re a golfer, and a consistent one at that. On any given day you want to predict where your score will be in two buckets: below par or over par.\n",
    "\n",
    "<p align='center'><img src='https://www.mastersindatascience.org/wp-content/uploads/sites/54/2022/05/tree-graphic.jpg'></p>\n",
    "\n",
    "While you are a consistent golfer, your score is dependent on a few sets of input variables. Wind speed, cloud cover and temperature all play a role. In addition, your score tends to deviate depending on whether or not you walk or ride a cart. And it deviates if you are golfing with friends or strangers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method to reduce the overfitting is training several decision trees independently, it isn't supposed to learn all the trees at once. But it is highly used the addition strategy: fix what we have learned, and add one new tree at a time.\n",
    "So the prediction will be: \n",
    "$$\\begin{split}\\hat{y}_i^{(0)} &= 0\\\\\n",
    "\\hat{y}_i^{(1)} &= f_1(x_i) = \\hat{y}_i^{(0)} + f_1(x_i)\\\\\n",
    "\\hat{y}_i^{(2)} &= f_1(x_i) + f_2(x_i)= \\hat{y}_i^{(1)} + f_2(x_i)\\\\\n",
    "&\\dots\\\\\n",
    "\\hat{y}_i^{(t)} &= \\sum_{k=1}^t f_k(x_i)= \\hat{y}_i^{(t-1)} + f_t(x_i)\\end{split}$$\n",
    "\n",
    "And the tree used for the next step, is the one who optimizes the following objective function: \n",
    "$$\\sum_{i=1}^n [g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i)] + \\omega(f_t)$$\n",
    "\n",
    "Where $g_i$, $h_i$, and the regularization term $\\omega(f)$ are defined as: \n",
    "$$\\begin{split}g_i &= \\partial_{\\hat{y}_i^{(t-1)}} l(y_i, \\hat{y}_i^{(t-1)})\\\\\n",
    "h_i &= \\partial_{\\hat{y}_i^{(t-1)}}^2 l(y_i, \\hat{y}_i^{(t-1)})\\\\\n",
    "\\omega(f) &= \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2\\end{split}$$\n",
    "\n",
    "Where $w$ is the vector scores on the leaves and T is the number of leaves.\n",
    "This becomes our optimization goal for the new tree. The important advantages of this definition is that the value of the objective function only depends on $g_i$ and $h_i$, and we can choose any loss function we want.\n",
    "\n",
    "In general, the best objective reduction we can obtain is given by\n",
    "$$\n",
    "\\begin{split} w_j^\\ast &= -\\frac{G_j}{H_j+\\lambda}\\\\\n",
    "\\text{obj}^\\ast &= -\\frac{1}{2} \\sum_{j=1}^T \\frac{G_j^2}{H_j+\\lambda} + \\gamma T \\quad(1)\\end{split}\n",
    "$$\n",
    "Where $G_j = \\sum_{i\\in I_j} g_i$ and $H_j = \\sum_{i\\in I_j} h_i$ ($I_j = \\{i|q(x_i)=j\\}$ is the set of indices of data points assigned to the $j$-th leaf).\n",
    "\n",
    "## Building the Trees\n",
    "The equation 1 measures how good a tree is, and we would like to do this for all the trees we have. Realistically it is intractable, so the optimization are made every level at a time. Every node is splitted using the following formula: \n",
    "$$\n",
    "Gain = \\frac{1}{2} \\left[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}\\right] - \\gamma\n",
    "$$\n",
    "\n",
    "The first addend represents the score on the new left leaf, the second represents the score on the new right leaf, the third represents the original leaf score, and the last is regularization on the additional leaf. \n",
    "\n",
    "Analysing the previous formula, it shows that if the gain is smaller than $\\gamma$, it is better not to add that branch. (**Pruning Method**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
